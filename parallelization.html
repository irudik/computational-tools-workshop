<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Parallelization</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ivan Rudik" />
    <script src="parallelization_files/header-attrs/header-attrs.js"></script>
    <link href="parallelization_files/remark-css/default.css" rel="stylesheet" />
    <link href="parallelization_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="parallelization_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link href="parallelization_files/tile-view/tile-view.css" rel="stylesheet" />
    <script src="parallelization_files/tile-view/tile-view.js"></script>
    <link href="parallelization_files/panelset/panelset.css" rel="stylesheet" />
    <script src="parallelization_files/panelset/panelset.js"></script>
    <script src="parallelization_files/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Parallelization
## Julia, R, Stata
### Ivan Rudik
### Computational tools for social scientists

---

exclude: true

```r
if (!require("pacman")) install.packages("pacman")
```

```
## Loading required package: pacman
```

```r
pacman::p_load(
  tidyverse, xaringanExtra, Ryacas, rlang
)
options(htmltools.dir.version = FALSE)
knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo &lt;- FALSE
  }
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
  options
})
```


```
## Warning: 'xaringanExtra::style_panelset' is deprecated.
## Use 'style_panelset_tabs' instead.
## See help("Deprecated")
```

```
## Warning in style_panelset_tabs(...): The arguments to `syle_panelset()` changed
## in xaringanExtra 0.1.0. Please refer to the documentation to update your slides.
```

&lt;style type="text/css"&gt;
/* custom.css */
.left-code {
  color: #777;
  width: 38%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 60%;
  float: right;
  padding-left: 1%;
}
.plot-callout {
  height: 225px;
  width: 450px;
  bottom: 5%;
  right: 5%;
  position: absolute;
  padding: 0px;
  z-index: 100;
}
.plot-callout img {
  width: 100%;
  border: 4px solid #23373B;
}
&lt;/style&gt;

---

# Why does parallelization matter?

Procedures in economics often require the execution of many independent jobs:

--

.hi-blue[Bootstrap:] estimate model on independent re-samples of the data

--

.hi-blue[Fixed effects estimation:] method underlying `reghdfe`, `lfe::felm` (method of alternating projections) has components that can be done in parallel

--

.hi-blue[Value function iteration:] set of independent Bellman-maximization problems on a state space grid

--

.hi-blue[Raster operations:] if you need to perform transformations, reprojections, etc

--

.hi-blue[Any data processing over multiple years, states:] These are probably independent, can be written as a function

---

# Why does parallelization matter?

These tasks can be significantly sped up by executing them in parallel

--

Doing things in parallel is different across languages, platforms

--

Here we will see how to do parallel computing in Stata, R, and Julia
  - everything on a 2 core laptop

---

# Stata

How do we parallelize in Stata?

Step 1: Buy StataMP
Step 2: Run StataMP

--

.pull-left[

![Stata is pricey](images/stata.png)
]

.pull-right[

![Stata is pricey](images/stata_speeds.png)

]

---

# Stata

Parallelization in Stata is *relatively* straightforward because you don't have to do anything

--

Many common functions are parallelized (e.g. reshape, replace, destring, sort)

--

Keep in mind you wont often get N times speed up with N cores

--

Stata also doesn't auto-parallelize some obvious ones like bootstrapping

--

You can get around this by parallelizing batch calls to Stata using `parallel` at [https://github.com/gvegayon/parallel](https://github.com/gvegayon/parallel)

--

Or just do it in serial, you don't need to parallelize everything

---

# R

You don't need to pay to parallelize in R (nice)

--

There's a lot of different ways to do it though

--

I'll just be going over two ways that use the `future` and `furrr` packages:
   - Loops
   - Apply/map

I'd generally recommend the `apply` and `map` family of functions over loops for parallelizing, it forces you to code in terms of functions and makes your code cleaner

--

Use loops when future iterations depend on previous iterations

---

# R

Required packages for the R code: `tidyverse, broom, tictoc, lfe, fixest, furrr, doFuture`

--

The GitHub repo `irudik/computational-tools-workshop` has everything ready to go with matching package versions using the `renv` package

--

What we will be doing in parallel is .hi-red[bootstrapping standard errors]

--

Main idea in case you forgot:

1. Resample dataset N times with replacement
2. Estimate the model on the N resampled datasets
3. Standard deviation of your estimate of interest across the N datasets is your estimated standard error

---

# R Step 1: Load packages

First you've gotta load your packages


```r
# Load packages via pacman package manager
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse, tictoc, doFuture,
  fixest, broom, furrr, renv
)
```

You can alternatively use the usual `library` loaders but `pacman` is nice and auto-installs new packages

---

# R Step 2: Initialize parallel processing infrastructure

Next, you need to let your compute know you're going to do parallel processing


```r
registerDoFuture()
plan(multiprocess)
```

```
## Warning: [ONE-TIME WARNING] Forked processing ('multicore') is disabled
## in future (&gt;= 1.13.0) when running R from RStudio, because it is
## considered unstable. Because of this, plan("multicore") will fall
## back to plan("sequential"), and plan("multiprocess") will fall back to
## plan("multisession") - not plan("multicore") as in the past. For more details,
## how to control forked processing or not, and how to silence this warning in
## future R sessions, see ?future::supportsMulticore
```

`registerDoFuture()` spins up the parallel computing infrastructure


`plan(multiprocess)` lets the computer know your plan for doing computing (serially, multicore, multithreaded, etc)

---

# R Step 3: Initialize RNG seed

For reproducibility, we need to set a seed for our random number generator


```r
set.seed(1234321)
```

This makes sure that we get the same result each time we (or someone else) run our code even though the re-sampling is done "randomly"

---

# R Step 4: Load in data

Read in the csv, this is just a dataset on biodiversity and economic development


```r
df &lt;- read_csv("data/final_panel.csv")
df
```

```
## # A tibble: 60,512 x 7
##    abundance  gdppc location_id  year temp_avg precip_avg state
##        &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;
##  1      3288 38225.           1  1992     5.66       162.    27
##  2      2220 42840.           1  1996     5.35       162.    27
##  3       542 28387.           7  1990    12.7        124.    35
##  4       536 28724.           7  1991    12.5        124.    35
##  5      1206 29172.           7  1992    12.5        124.    35
##  6      1581 30144.           7  1993    12.6        123.    35
##  7       863 30986.           7  1994    12.8        122.    35
##  8      1196 31574.           7  1995    13.0        121.    35
##  9       440 31941.           7  1996    13.2        120.    35
## 10      1446 32522.           7  1997    13.4        121.    35
## # â€¦ with 60,502 more rows
```

---

# R Step 5: Define parameters

Next we need to decide how many bootstrap samples we want, and we need to initialize a list for the execution times, and a data.frame for the output


```r
# Number of bootstrap resamples
N &lt;- 200

# initialize list of times
times &lt;- list()

# initialize storage df
bootstrap_samples &lt;- tibble()
```

---

# R Step 6: Define function

Next we need to define our bootstrap function for estimation


```r
bootstrap_ses &lt;- function(N) {
  df %&gt;%
    dplyr::sample_frac(replace = TRUE) %&gt;%
    fixest::feols(I(log(abundance)) ~ I(log(gdppc)) + temp_avg + precip_avg + interact(year, state) |
                    location_id + year, data = .) %&gt;%
    broom::tidy() %&gt;%
    dplyr::filter(stringr::str_detect(term, "gdppc")) %&gt;%
    dplyr::select(estimate)
}
```

This function:

1. resamples the dataset with replacement
2. estimates a fixed effect model with `fixest::feols`
3. recovers the coefficient on `log(gdppc)`


---

# R Step 7: Execute the function

First lets run it sequentially to get a sense of the baseline:


```r
tic()
serial_feols &lt;- replicate(N, bootstrap_ses(N)) %&gt;%
  unlist() %&gt;%
  as_tibble() %&gt;%
  mutate(type = "serial")
time &lt;- toc()
```

```
## 101.327 sec elapsed
```

```r
times[["serial_feols"]] &lt;- time$toc - time$tic
```

---

# R Step 7: Execute the function

Next, lets run it in parallel using a parallel for loop


```r
tic()
dopar_feols &lt;- foreach(n = seq(N)) %dopar% {
  bootstrap_ses(n)
} %&gt;%
  unlist() %&gt;%
  as_tibble() %&gt;%
  mutate(type = "dopar")
time &lt;- toc()
```

```
## 53.856 sec elapsed
```

```r
times[["dopar_feols"]] &lt;- time$toc - time$tic
```

---

# R Step 7: Execute the function

Next, lets run it in parallel using `furrr::future_map_dfr`


```r
tic()
furrr_feols &lt;- future_map_dfr(
  seq(N),
  bootstrap_ses,
  .options = future_options(globals = "df")
  ) %&gt;%
  unlist() %&gt;%
  as_tibble() %&gt;%
  mutate(type = "furrr")
time &lt;- toc()
```

```
## 49.93 sec elapsed
```

```r
times[["furrr_feols"]] &lt;- time$toc - time$tic
```

---

# Now lets check the output

Now lets look at what we got:


```r
times
```

```
## $serial_feols
## elapsed 
## 101.327 
## 
## $dopar_feols
## elapsed 
##  53.856 
## 
## $furrr_feols
## elapsed 
##   49.93
```

Parallelization across 2 cores gets almost exactly 2x speed up

---

# Now lets check the output

Estimation is relatively expensive so the fixed costs of parallelizing aren't that big relative to the speed gains

`furrr` is generally slightly faster than `dopar` in addition to being better coding

--

Clone the github repo and use `parallelization_server.R` on the BioHPC servers to see if `\(#\)` cores speed-up holds more generally

---

# Now lets check the output


```r
bind_rows(serial_feols, dopar_feols, furrr_feols) %&gt;%
  group_by(type) %&gt;%
  summarise(sd(value))
```

```
## `summarise()` ungrouping output (override with `.groups` argument)
```

```
## # A tibble: 3 x 2
##   type   `sd(value)`
##   &lt;chr&gt;        &lt;dbl&gt;
## 1 dopar        0.226
## 2 furrr        0.239
## 3 serial       0.236
```

We get similar standard error estimates

---

# Julia

You also don't need to pay to parallelize in R (nice)

--

There's two main ways to parallelize in Julia, both have pros and cons:
  - `pmap`
  - `@distributed`

--

Rule of thumb: use `pmap` with big jobs, use `@distributed` with smaller jobs

---

# Julia: pmap vs distributed

`pmap` is a parallel map

`@distributed` makes a parallel for loop like `%dopar%` in R

--

What's the difference?

--

`@distributed` takes a quick and dirty approach: it just assigns the same number of jobs to each worker

--

This can cause inefficiencies if you have jobs with different execution times, leaving you hanging with one worker with several jobs left to do when they could be spread out over many workers

--

The benefit is it's quick: just send the jobs to the workers and wait

---

# Julia: pmap vs distributed

`pmap` does .hi-blue[load balancing / dynamic scheduling:] it assigns jobs to workers as they complete jobs

--

This makes sure workers are always busy and not idle

--

But there's a fixed cost communication overhead with load balancing: if you have small easy jobs that are similar size, this will dominate the gains from load balancing

---

# Julia

Required packages for the Julia code: `Pkg, Distributed, Random, DataFrames, CSVFiles, FixedEffectModels, SharedArrays, Statistics`

--

The GitHub repo `irudik/computational-tools-workshop` has everything ready to go with matching package versions using the `renv` package

--

Again, we will be .hi-red[bootstrapping standard errors] in parallel


---

# Julia Step 1: Load packages

First you've gotta load your packages onto each worker


```julia
using Pkg
Pkg.activate(".")
Pkg.instantiate()
using Distributed, Random
```

Julia has very nice built-in package manager, making it easy to generate reproducible code


---

# Julia Step 2: Instantiate parallel processing infrastructure

```julia
# add 1 worker
addprocs(1)
```

```
## 1-element Array{Int64,1}:
##  2
```

```julia

@everywhere using Pkg
@everywhere Pkg.activate(".")
```

```
##       From worker 2:	 Activating environment at `~/Desktop/git/computational-tools-workshop/Project.toml`
```

```julia
@everywhere Pkg.instantiate()
@everywhere using Queryverse, FixedEffectModels, SharedArrays
```

A quirk of Julia is you need to start off loading the packages onto each worker using `@everywhere`

---

# Julia Step 3: Initialize RNG seed


```julia
Random.seed!(1234321)
```

```
## MersenneTwister(UInt32[0x0012d591], Random.DSFMT.DSFMT_state(Int32[-1066020669, 1073631810, 397127531, 1072701603, -312796895, 1073626997, 1020815149, 1073320576, 650048908, 1073512247  â€¦  -352178910, 1073735534, 1816227101, 1072823316, -1468787611, -2121692099, 358864500, -310934288, 382, 0]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  â€¦  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], UInt128[0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000  â€¦  0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000], 1002, 0)
```

This makes sure our results are reproducible

---

# Julia Step 4: Load data


```julia
df = DataFrame(load("data/final_panel.csv"))
```

```
## 60512Ã—7 DataFrame. Omitted printing of 1 columns
## â”‚ Row   â”‚ abundance â”‚ gdppc   â”‚ location_id â”‚ year  â”‚ temp_avg â”‚ precip_avg â”‚
## â”‚       â”‚ Float64   â”‚ Float64 â”‚ Int64       â”‚ Int64 â”‚ Float64  â”‚ Float64    â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ 1     â”‚ 3288.0    â”‚ 38225.3 â”‚ 1           â”‚ 1992  â”‚ 5.66217  â”‚ 162.065    â”‚
## â”‚ 2     â”‚ 2220.0    â”‚ 42840.3 â”‚ 1           â”‚ 1996  â”‚ 5.35117  â”‚ 161.835    â”‚
## â”‚ 3     â”‚ 542.0     â”‚ 28386.8 â”‚ 7           â”‚ 1990  â”‚ 12.6512  â”‚ 123.907    â”‚
## â”‚ 4     â”‚ 536.0     â”‚ 28723.7 â”‚ 7           â”‚ 1991  â”‚ 12.5427  â”‚ 123.526    â”‚
## â”‚ 5     â”‚ 1206.0    â”‚ 29172.2 â”‚ 7           â”‚ 1992  â”‚ 12.4691  â”‚ 123.882    â”‚
## â”‚ 6     â”‚ 1581.0    â”‚ 30144.2 â”‚ 7           â”‚ 1993  â”‚ 12.5805  â”‚ 122.682    â”‚
## â”‚ 7     â”‚ 863.0     â”‚ 30985.9 â”‚ 7           â”‚ 1994  â”‚ 12.7852  â”‚ 121.862    â”‚
## â‹®
## â”‚ 60505 â”‚ 41881.5   â”‚ 40464.5 â”‚ 30185       â”‚ 2006  â”‚ 10.4133  â”‚ 122.89     â”‚
## â”‚ 60506 â”‚ 34425.5   â”‚ 40821.8 â”‚ 30185       â”‚ 2007  â”‚ 10.3946  â”‚ 123.305    â”‚
## â”‚ 60507 â”‚ 29837.7   â”‚ 41114.4 â”‚ 30185       â”‚ 2008  â”‚ 10.3504  â”‚ 122.486    â”‚
## â”‚ 60508 â”‚ 7102.4    â”‚ 40369.0 â”‚ 30185       â”‚ 2009  â”‚ 10.2743  â”‚ 121.816    â”‚
## â”‚ 60509 â”‚ 24805.6   â”‚ 41093.7 â”‚ 30185       â”‚ 2010  â”‚ 10.1337  â”‚ 121.996    â”‚
## â”‚ 60510 â”‚ 21776.8   â”‚ 41433.9 â”‚ 30185       â”‚ 2011  â”‚ 10.1011  â”‚ 120.375    â”‚
## â”‚ 60511 â”‚ 22725.9   â”‚ 42033.8 â”‚ 30185       â”‚ 2012  â”‚ 10.1315  â”‚ 120.391    â”‚
## â”‚ 60512 â”‚ 43839.5   â”‚ 42346.5 â”‚ 30185       â”‚ 2013  â”‚ 10.0043  â”‚ 120.174    â”‚
```

`Queryverse` is the Julia-equivalent of tidyverse for R

---

# Julia Step 5: Define parameters


```julia
N = 200
```

```
## 200
```

```julia
estimates_serial = Vector{Float64}(undef, N)
```

```
## 200-element Array{Float64,1}:
##  7.85138442677e-313
##  8.487983181e-314
##  9.12458190135e-313
##  1.039777937805e-312
##  1.08221785364e-312
##  8.487983183e-314
##  8.487983166e-314
##  1.31563739069e-312
##  1.4641770961e-312
##  1.54905692775e-312
##  â‹®
##  1.5e-323
##  4.74e-322
##  1.5e-323
##  4.8e-322
##  1.5e-323
##  4.64e-322
##  4.64e-322
##  4.4e-322
##  4.4e-322
```

```julia
estimates_distrib = SharedArray{Float64}(N)
```

```
## 200-element SharedArray{Float64,1}:
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  â‹®
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
##  0.0
```

We need to initialize `Array`s for the serial and `@distributed` results, not for `pmap`

---

# Julia Step 6: Define function

Now we define our bootstrap function


```julia
@everywhere function bootstrap_ses(df)
    bootstrap_df = df[sample(axes(df, 1), nrow(df); replace = true), :]
    reg_output = reg(bootstrap_df,
        @formula(log(abundance) ~ log(gdppc) + temp_avg + precip_avg + fe(state)&amp;year + fe(location_id) + fe(year)))

    return reg_output.coef[1]
end
```

The `@everywhere` macro makes sure its defined on all workers

---

# Julia Step 7: Execute function (pre-compile)

In Julia you often get massive speed gains by executing your function once before doing your main work

--

This is because it pre-compiles the function



---

# Julia Step 7: Execute function

First lets run it sequentially


```julia
@time for i in 1:N
  estimates_serial[i] = bootstrap_ses(df)
end
```

```
##  83.125242 seconds (24.58 M allocations: 6.040 GiB, 1.30% gc time)
```


---

# Julia Step 7: Execute function

First lets use `pmap` to do load-balancing


```julia
@time estimates_pmap = pmap(bootstrap_ses, [df for i in 1:N])
```

```
##  83.641638 seconds (91.90 k allocations: 4.361 MiB)
```

```
## 200-element Array{Float64,1}:
##  -1.4937577825341875
##  -1.8060739464891322
##  -2.0008244359390037
##  -2.04720232085125
##  -2.108315487860598
##  -1.9478042301684217
##  -1.893527709363631
##  -1.864192063549577
##  -1.7807967421686677
##  -1.6740688665267969
##   â‹®
##  -1.5472486974498683
##  -2.0372254099407288
##  -1.7536129616492335
##  -2.0123926162507093
##  -1.6611045662504702
##  -1.7025102863085022
##  -1.6536189809149595
##  -1.6296410998199775
##  -1.6739840921179572
```


---

# Julia Step 7: Execute function

Last lets use `@distributed` to do it quick and dirty


```julia
@time @sync @distributed for i in 1:N
  estimates_distrib[i] = bootstrap_ses(df)
end
```

```
##       From worker 2:	
Demeaning Variables... 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       |  ETA: 0:00:00[K
Demeaning Variables...100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:01[K
## 115.439315 seconds (161.00 k allocations: 8.321 MiB)
```

```
## Task (done) @0x00000001418f5d50
```


---

# Now lets check the output


```julia
results = DataFrame(serial = estimates_serial, pmap = estimates_pmap, distrib = estimates_distrib);
describe(results, :standard_deviation =&gt; std)
```

```
## 3Ã—2 DataFrame
## â”‚ Row â”‚ variable â”‚ standard_deviation â”‚
## â”‚     â”‚ Symbol   â”‚ Float64            â”‚
## â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ 1   â”‚ serial   â”‚ 0.257461           â”‚
## â”‚ 2   â”‚ pmap     â”‚ 0.241915           â”‚
## â”‚ 3   â”‚ distrib  â”‚ 0.235289           â”‚
```

--

Clone the github repo and use `parallelization_server.jl` on the BioHPC servers to see if `\(#\)` cores speed-up holds more generally
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
